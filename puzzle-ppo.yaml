puzzle-ppo:
    env: puzzle-v0
    run: PPO
    stop:
        # time_total_s: 60
        # timesteps_total: 1000,
        #episode_reward_max: 1.0
        training_iteration: 50
    config:
        #lambda: 0.95
        # kl_coeff: 0.5
        # clip_rewards: True
        # clip_param: 0.1
        # vf_clip_param: 10.0
        # entropy_coeff: 0.01
        # train_batch_size: 5000
        # sample_batch_size: 100
        # sgd_minibatch_size: 500
        # num_sgd_iter: 10
        num_workers: 6
        num_gpus: 1